# ğŸ˜ï¸ House Prices Prediction

This repository contains a machine learning project built as part of my ML learning journey.  
The goal is to predict house sale prices using structured real-estate data from Kaggleâ€™s  
**House Prices: Advanced Regression Techniques** competition. 

The project focuses on building a **professional end-to-end tabular ML pipeline**, not just training a model in a notebook.

---

ğŸ“Œ Overview  

Task: Regression â€” predict `SalePrice`  
Dataset: [Kaggle House Prices competition](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data)  
Approach: Feature engineering + leakage-safe preprocessing + gradient boosting  
Model: XGBoost Regressor  
Evaluation: RMSE / MAE / RÂ² on a validation split  

During exploration (in the `notebooks/` directory), multiple regression models and preprocessing strategies were tested and compared.  
The best-performing pipeline was then selected and implemented in the final training scripts.

---

ğŸ“‚ Structure

```
project-root/
â”‚
â”œâ”€â”€ data/
â”‚ â”œâ”€â”€ raw/ # Original Kaggle CSV files
â”‚ â”œâ”€â”€ processed/
â”‚
â”œâ”€â”€ notebooks/ # Exploration & experimentation
â”‚
â”œâ”€â”€ src/
â”‚
â”œâ”€â”€ utils/
â”‚ â”œâ”€â”€ lotfront_objects/ # Encoder, scaler, imputer for LotFrontage
â”‚ â””â”€â”€ preprocessor_objects/
â”‚  â””â”€â”€ preprocessor.pkl
â”‚
â”œâ”€â”€ model/
â”‚ â””â”€â”€ xgb_house_price_model.pkl
â”‚
â””â”€â”€ README.md
```

---

ğŸ§  Modeling & Feature Engineering  

Initial cleaning:

- Convert `MSSubClass` to categorical
- Drop low-information columns
- Fill missing information based on documentation
- Logic-based filling for missing values

LotFrontage imputation:

- Ordinal encoding of neighborhood-related features
- KNNImputer with distance weighting
- Fitted objects saved for inference reuse

Full preprocessing:

- Numerical features â†’ StandardScaler
- Categorical features â†’ OneHotEncoder
- Implemented via ColumnTransformer and serialized to disk

Models explored in notebooks:

- Linear Regression / Ridge / Lasso  
- Random Forest  
- Gradient Boosting  
- XGBoost  

Final model:

- XGBoost Regressor  
- Selected based on validation RMSE and overall generalization

---

ğŸš€ Whatâ€™s Coming Next  

The next step for this project is to complete the full Kaggle competition workflow by:

- Running the trained pipeline on Kaggleâ€™s `test.csv` dataset  
- Applying the same cleaning, imputation, and preprocessing steps used during training  
- Generating predictions with the saved XGBoost model  
- Creating a Kaggle-ready `submission.csv` file
  
---

â–¶ï¸ How to Run  

From the project root:

Preprocess the data:

```bash
python src/Preprocess.py
```

Train the model:

```bash
python src/train.py
```

The trained model is saved to: `model/xgb_house_price_model.pkl`