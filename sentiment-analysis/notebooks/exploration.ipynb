{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5556b76f-4d3a-4cd9-b9a8-28602e1f3841",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "from string import punctuation\n",
    "from sklearn.utils import shuffle\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6646487a-c43c-41cd-be83-6b42a8ef1457",
   "metadata": {},
   "outputs": [],
   "source": [
    "translator = str.maketrans('', '', punctuation)\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words.remove('not')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd1d7e53-3bf2-4d14-b05e-082809dab26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ignore_words = {'film', 'movie', 'one', 'even', 'would', 'time', 'get', 'story', 'could',\n",
    "    'plot', 'make', 'see', 'also', 'way', 'little', 'well', 'people', 'never',\n",
    "    'know', 'two', 'another', 'big', 'made', 'go', 'back', 'around', 'going',\n",
    "    'think', 'still', 'characters', 'first', 'character', 'scene', 'scenes',\n",
    "    'films', 'movies', 'man', 'new', 'may', 'take', 'almost', 'every', 'things',\n",
    "    'real', 'comes', 'come', 'fact', 'last', 'point', 'plays', 'played', 'role',\n",
    "    'years', 'john', 'audience', 'us'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "52fd70e6-3ba6-40c7-ba8a-386148278216",
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_documents = []\n",
    "max_len_negative = 0\n",
    "for file in os.listdir('../data/raw/neg'):\n",
    "    with open ('../data/raw/neg/' + file) as f:\n",
    "        text = f.read()\n",
    "        text = text.lower()\n",
    "        tokens = word_tokenize(text)\n",
    "        tokens = [t.translate(translator) for t in tokens]\n",
    "        tokens = [t for t in tokens if t]\n",
    "        tokens = [t for t in tokens if t not in stop_words and t not in ignore_words]\n",
    "        if len(tokens) > max_len_negative:\n",
    "            max_len_negative = len(tokens)\n",
    "        negative_documents.append(' '.join(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "28e93190-7bdc-4105-ac33-a2b81ceeddec",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_documents = []\n",
    "max_len_positive= 0\n",
    "for file in os.listdir('../data/raw/pos'):\n",
    "    with open ('../data/raw/pos/' + file) as f:\n",
    "        text = f.read()\n",
    "        text = text.lower()\n",
    "        tokens = word_tokenize(text)\n",
    "        tokens = [t.translate(translator) for t in tokens]\n",
    "        tokens = [t for t in tokens if t]\n",
    "        tokens = [t for t in tokens if t not in stop_words and t not in ignore_words]\n",
    "        if len(tokens) > max_len_positive:\n",
    "            max_len_positive = len(tokens)\n",
    "        positive_documents.append(' '.join(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d22ce333-f1fd-443f-954c-5e483a88a748",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "879"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(max_len_positive, max_len_negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc8655f2-c5ea-4ee3-8be5-2dbd2374374b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neg: [('nt', 3442), ('not', 2694), ('like', 1836), ('good', 1128), ('bad', 1021), ('much', 998), ('really', 787), ('action', 606), ('director', 595), ('better', 530), ('end', 527), ('something', 525), ('seems', 502), ('work', 494), ('best', 493), ('nothing', 493), ('life', 492), ('many', 489), ('enough', 481), ('script', 477), ('thing', 449), ('love', 448), ('funny', 444), ('gets', 441), ('look', 436), ('actually', 436), ('makes', 431), ('though', 430), ('however', 422), ('comedy', 412), ('least', 411), ('say', 400), ('great', 396), ('minutes', 383), ('since', 381), ('long', 375), ('acting', 369), ('ca', 368), ('actors', 366), ('seen', 366), ('guy', 364), ('world', 361), ('find', 360), ('cast', 360), ('old', 349), ('original', 348), ('might', 343), ('show', 342), ('right', 341), ('ever', 338), ('goes', 337), ('anything', 337), ('performance', 336), ('course', 333), ('part', 332), ('interesting', 332), ('lot', 328), ('year', 326), ('effects', 324), ('trying', 315), ('give', 313), ('away', 312), ('although', 307), ('without', 304), ('young', 302), ('dialogue', 298), ('three', 296), ('series', 294), ('want', 293), ('far', 292), ('must', 291), ('screen', 290), ('pretty', 288), ('watch', 286), ('instead', 284), ('got', 282), ('given', 281), ('special', 280), ('looks', 280), ('star', 279), ('reason', 279), ('rather', 278), ('woman', 277), ('day', 276), ('takes', 272), ('seem', 271), ('place', 271), ('kind', 269), ('whole', 268), ('watching', 265), ('hard', 264), ('money', 261), ('worst', 259), ('probably', 258), ('sure', 257), ('hollywood', 257), ('actor', 254), ('becomes', 250), ('unfortunately', 249), ('along', 249)]\n",
      "pos: [('not', 2986), ('nt', 2775), ('like', 1719), ('good', 1193), ('much', 1027), ('life', 994), ('best', 809), ('many', 780), ('really', 776), ('great', 745), ('love', 649), ('world', 646), ('however', 567), ('makes', 561), ('performance', 549), ('seen', 537), ('seems', 530), ('something', 528), ('end', 521), ('director', 517), ('work', 515), ('though', 510), ('although', 488), ('action', 472), ('right', 442), ('young', 437), ('family', 429), ('enough', 426), ('gets', 421), ('find', 420), ('quite', 417), ('star', 405), ('cast', 404), ('say', 402), ('ever', 400), ('actually', 398), ('year', 398), ('takes', 396), ('look', 394), ('without', 393), ('comedy', 390), ('better', 387), ('funny', 387), ('since', 387), ('long', 381), ('show', 380), ('old', 380), ('yet', 373), ('part', 371), ('original', 364), ('thing', 360), ('always', 358), ('bad', 356), ('lot', 349), ('job', 347), ('day', 343), ('rather', 342), ('american', 341), ('place', 339), ('away', 339), ('screen', 338), ('three', 337), ('actors', 331), ('fun', 325), ('gives', 325), ('picture', 325), ('sense', 324), ('bit', 321), ('wife', 321), ('times', 320), ('acting', 320), ('far', 320), ('must', 318), ('watch', 316), ('men', 316), ('making', 315), ('course', 314), ('nothing', 309), ('goes', 309), ('home', 308), ('become', 306), ('effects', 305), ('seem', 303), ('interesting', 301), ('together', 299), ('hollywood', 298), ('script', 298), ('set', 297), ('everything', 295), ('especially', 293), ('black', 292), ('might', 291), ('father', 291), ('help', 288), ('special', 285), ('music', 283), ('different', 283), ('anything', 282), ('along', 281), ('instead', 281)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "neg_tokens = [t for doc in negative_documents for t in doc.split()]\n",
    "neg_counter = Counter(neg_tokens)\n",
    "most_common_neg = neg_counter.most_common(100)\n",
    "print(\"neg:\", most_common_neg)\n",
    "\n",
    "pos_tokens = [t for doc in positive_documents for t in doc.split()]\n",
    "pos_counter = Counter(pos_tokens)\n",
    "most_common_pos = pos_counter.most_common(100)\n",
    "print(\"pos:\", most_common_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e1702ac-5c6b-48e1-b9d1-935277ed2936",
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_documents = shuffle(negative_documents, random_state=42)\n",
    "positive_documents = shuffle(positive_documents, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dd4db343-bc46-494e-83f4-54a594ee7a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = Path('../data/processed')\n",
    "for split in ['train', 'val', 'test']:\n",
    "    (BASE_DIR / split).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c3071c06-20cf-4674-a529-4316b582581d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.DataFrame(negative_documents[:700] + positive_documents[:700])\n",
    "y_train = pd.Series([0 for _ in range(700)] + [1 for _ in range(700)])\n",
    "X_train, y_train = shuffle(X_train, y_train, random_state=42)\n",
    "X_train.to_parquet('../data/processed/train/X.parquet')\n",
    "y_train.to_frame().to_parquet('../data/processed/train/y.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "31457fda-516f-4bb8-aa9d-20772b42fc8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val = pd.DataFrame(negative_documents[700:850] + positive_documents[700:850])\n",
    "y_val = pd.Series([0 for _ in range(150)] + [1 for _ in range(150)])\n",
    "X_val, y_val = shuffle(X_val, y_val, random_state=42)\n",
    "X_val.to_parquet('../data/processed/val/X.parquet')\n",
    "y_val.to_frame().to_parquet('../data/processed/val/y.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a626ae17-4e62-4b85-bead-ce83e6483aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = pd.DataFrame(negative_documents[850:] + positive_documents[850:])\n",
    "y_test = pd.Series([0 for _ in range(150)] + [1 for _ in range(150)])\n",
    "X_test, y_test = shuffle(X_test, y_test, random_state=42)\n",
    "X_test.to_parquet('../data/processed/test/X.parquet')\n",
    "y_test.to_frame().to_parquet('../data/processed/test/y.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "b6dd5733-38c1-4c2d-8db7-cfa3875d61bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    max_features=20000,\n",
    "    ngram_range=(1, 2),\n",
    "    min_df=2\n",
    ")\n",
    "\n",
    "X_train_vec = vectorizer.fit_transform(X_train[0])\n",
    "X_val_vec = vectorizer.transform(X_val[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "e263ad32-c8bd-4bd7-8a25-5a010c4da311",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8366666666666667\n",
      "0.839344262295082\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg = LogisticRegression(max_iter=1000, random_state=42)\n",
    "logreg.fit(X_train_vec, y_train)\n",
    "y_pred = logreg.predict(X_val_vec)\n",
    "print(accuracy_score(y_val, y_pred))\n",
    "print(f1_score(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "0a3bf9e8-232c-49fc-8eaf-e537f2119dcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8333333333333334\n",
      "0.8366013071895425\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "svm = LinearSVC(random_state=42)\n",
    "svm.fit(X_train_vec, y_train)\n",
    "y_pred = svm.predict(X_val_vec)\n",
    "print(accuracy_score(y_val, y_pred))\n",
    "print(f1_score(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "2e2a987e-22cf-40eb-8449-4b4073e7972e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8033333333333333\n",
      "0.8039867109634552\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "nb = MultinomialNB()\n",
    "nb.fit(X_train_vec, y_train)\n",
    "y_pred = nb.predict(X_val_vec)\n",
    "print(accuracy_score(y_val, y_pred))\n",
    "print(f1_score(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "8882892a-760b-452c-96f5-e8a1631a1a8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.83      0.85       150\n",
      "           1       0.84      0.87      0.85       150\n",
      "\n",
      "    accuracy                           0.85       300\n",
      "   macro avg       0.85      0.85      0.85       300\n",
      "weighted avg       0.85      0.85      0.85       300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_test_vec = vectorizer.transform(X_test[0])\n",
    "best_model = svm \n",
    "test_preds = best_model.predict(X_test_vec)\n",
    "print(classification_report(y_test, test_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "4181d441-2d1a-4052-a04a-87815bbe8e44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.63\n",
      "0.607773851590106\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dt = DecisionTreeClassifier(criterion='entropy', random_state=42)\n",
    "dt.fit(X_train_vec, y_train)\n",
    "y_pred = dt.predict(X_val_vec)\n",
    "print(accuracy_score(y_val, y_pred))\n",
    "print(f1_score(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "3450f727-609f-4f12-92da-8429186d07f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8\n",
      "0.7902097902097902\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier(n_estimators=100, criterion='entropy', random_state=42)\n",
    "rf.fit(X_train_vec, y_train)\n",
    "y_pred = rf.predict(X_val_vec)\n",
    "print(accuracy_score(y_val, y_pred))\n",
    "print(f1_score(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "5d2d19fb-149b-4594-9914-8cf97c838eb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.86      0.82       150\n",
      "           1       0.85      0.77      0.81       150\n",
      "\n",
      "    accuracy                           0.82       300\n",
      "   macro avg       0.82      0.82      0.82       300\n",
      "weighted avg       0.82      0.82      0.82       300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_test_vec = vectorizer.transform(X_test[0])\n",
    "test_preds = rf.predict(X_test_vec)\n",
    "print(classification_report(y_test, test_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "d688d2db-32f3-47b0-8b30-1a9499b859b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 45 candidates, totalling 225 fits\n",
      "Best params: {'tfidf__max_features': 20000, 'tfidf__min_df': 2, 'tfidf__ngram_range': (1, 1)}\n",
      "Best CV score: 0.8493085219206964\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    (\"tfidf\", TfidfVectorizer()),\n",
    "    (\"clf\", LinearSVC())\n",
    "])\n",
    "\n",
    "param_grid = {\n",
    "    \"tfidf__max_features\": [2000, 5000, 10000, 15000, 20000],\n",
    "    \"tfidf__ngram_range\": [(1,1), (1,2), (1,3)],\n",
    "    \"tfidf__min_df\": [2, 5, 10],\n",
    "}\n",
    "\n",
    "\n",
    "grid = GridSearchCV(\n",
    "    pipeline,\n",
    "    param_grid,\n",
    "    cv=5,\n",
    "    scoring=\"f1\",\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "grid.fit(X_train[0], y_train)\n",
    "\n",
    "print(\"Best params:\", grid.best_params_)\n",
    "print(\"Best CV score:\", grid.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "ad9a3c26-9451-4d2f-8cab-fd2e6c664ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer (\n",
    "    max_features=10000,\n",
    "    ngram_range=(1,1),\n",
    "    min_df=5\n",
    ")\n",
    "\n",
    "X_train_vec = vectorizer.fit_transform(X_train[0])\n",
    "X_val_vec = vectorizer.transform(X_val[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "7058c084-5974-446c-9216-ae36890095ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6233333333333333\n",
      "0.6062717770034843\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dt = DecisionTreeClassifier(criterion='entropy', random_state=42)\n",
    "dt.fit(X_train_vec, y_train)\n",
    "y_pred = dt.predict(X_val_vec)\n",
    "print(accuracy_score(y_val, y_pred))\n",
    "print(f1_score(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "81837400-7561-4f0f-ad3f-55a3337d940d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.85\n",
      "0.8464163822525598\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier(n_estimators=100, criterion='entropy', random_state=42)\n",
    "rf.fit(X_train_vec, y_train)\n",
    "y_pred = rf.predict(X_val_vec)\n",
    "print(accuracy_score(y_val, y_pred))\n",
    "print(f1_score(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "c5b13225-cda9-4b66-a351-3b6372f75918",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.85      0.82       150\n",
      "           1       0.84      0.79      0.82       150\n",
      "\n",
      "    accuracy                           0.82       300\n",
      "   macro avg       0.82      0.82      0.82       300\n",
      "weighted avg       0.82      0.82      0.82       300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_test_vec = vectorizer.transform(X_test[0])\n",
    "test_preds = rf.predict(X_test_vec)\n",
    "print(classification_report(y_test, test_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f699f4e7-bc33-4822-8c3e-4ccbf76ffa33",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
