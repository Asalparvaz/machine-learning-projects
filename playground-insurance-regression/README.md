# ğŸ›¡ï¸ Insurance Premium Prediction

This repository contains a machine learning project built as part of my ML learning journey.  
The goal is to predict insurance premium amounts using structured customer and policy data from Kaggle.  

ğŸ“Š **Current Kaggle public leaderboard score:** 1.04511

---

## ğŸ“Œ Overview

- **Task:** Regression â€” predict Premium Amount  
- **Dataset:** Kaggle Insurance Premium Prediction competition  
- **Approach:** Feature engineering + leakage-safe preprocessing + log-transformed boosting  
- **Model:** XGBoost Regressor (wrapped with log-target transformation)  
- **Evaluation Metrics:** RMSLE, MAE, RMSE, RÂ² on a validation split  

During exploration, multiple preprocessing strategies and modeling ideas were tested. The best-performing logic was consolidated into **custom sklearn-compatible transformers** and final **training/inference pipelines**.

---

## ğŸ“‚ Project Structure
```
project-root/
â”‚
â”œâ”€â”€ data/
â”‚ â”œâ”€â”€ raw/ # Original Kaggle CSV files
â”‚ â”œâ”€â”€ processed/
â”‚ â”œâ”€â”€ train/
â”‚ â””â”€â”€ val/
â”‚
â”œâ”€â”€ src/
â”‚ â”œâ”€â”€ CustomPreprocessor.py
â”‚ â”œâ”€â”€ LogXGBRegressor.py
â”‚ â”œâ”€â”€ Pipeline.py
â”‚ â”œâ”€â”€ Train.py
â”‚ â””â”€â”€ Inference.py
â”‚
â”œâ”€â”€ submission/
â”‚ â””â”€â”€ submission.csv
```


---

## ğŸ§  Modeling & Feature Engineering

### ğŸ”¹ CustomPreprocessor

A custom sklearn transformer is used to ensure **leakage-safe, reusable preprocessing**:

- **Median imputation** for:  
  `Age`, `Vehicle_Age`, `Health_Score`, `Previous_Claims`, `Credit_Score`, `Insurance_Duration`
- **Income handling:**  
  - Median imputation using customers with premiums in the 900â€“960 range  
  - Clipping at 1st and 99th percentiles
- **Missing-value indicator flags:**  
  `Marital_Status_Missing`, `Customer_Feedback_Missing`, `Income_Missing`, `Health_Score_Missing`
- **Date parsing:** Extract year and month from `Policy_Start_Date`

### ğŸ”¹ Feature Engineering

- Interaction features:
  - `Income_x_CreditScore`
  - `Income_x_HealthScore`
  - `CreditScore_x_HealthScore`
  - `Income_div_Dependents`
- Low-importance categorical or noisy columns are dropped based on experimentation.

### ğŸ”¹ Full Preprocessing Pipeline

Implemented entirely with sklearn objects:

1. Column name cleaning  
2. CustomPreprocessor  
3. Feature engineering  
4. Column dropping  
5. ColumnTransformer:
   - Numerical features â†’ `StandardScaler`
   - Remaining features â†’ passthrough  

All steps are **fit only on training data** and reused safely during inference.

---

## ğŸš€ Next Steps

Planned improvements:
- Hyperparameter tuning with cross-validation
-- SHAP-based feature importance analysis
- Error analysis on under/over-predicted samples
- Additional interaction features
- Climbing the leaderboard ğŸ“ˆ

---

## â–¶ï¸ How to Run

### ğŸ”¹ Train & Evaluate
``` bash
python src/Train.py
```
This will:
- Load `train.csv`
- Fit the preprocessing pipeline
- Transform train/validation splits
- Save NumPy arrays to `data/processed/`
Outputs:
    RMSLE, MAE, RMSE, RÂ² on validation set

### ğŸ”¹ Generate Kaggle Submission

```bash 
python src/Inference.py
```
This will:
- Fit the full pipeline on training data
- Load Kaggleâ€™s `test.csv`
- Apply preprocessing
- Generate predictions
- Save `submission/submission.csv` ready for upload